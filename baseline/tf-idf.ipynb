{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa5a772",
   "metadata": {},
   "source": [
    "refer to https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b40270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # information retrieval technique, similar to BoW\n",
    "\n",
    "cats = [\n",
    "    \"alt.atheism\",\n",
    "    \"sci.space\",\n",
    "]\n",
    "chosen_seed = 200\n",
    "\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode(\"utf-8\")) for s in docs) / 1e6\n",
    "\n",
    "\n",
    "def load_dataset(verbose=False, remove=()):\n",
    "    \"\"\"Load and vectorize the 20 newsgroups dataset.\"\"\"\n",
    "\n",
    "    data_train = fetch_20newsgroups(\n",
    "        subset=\"train\",\n",
    "        categories=cats,\n",
    "        shuffle=True,\n",
    "        random_state=chosen_seed,\n",
    "        remove=('headers', 'footers', 'quotes'),\n",
    "    )\n",
    "\n",
    "    data_test = fetch_20newsgroups(\n",
    "        subset=\"test\",\n",
    "        categories=cats,\n",
    "        shuffle=True,\n",
    "        random_state=chosen_seed,\n",
    "        remove=('headers', 'footers', 'quotes'),\n",
    "    )\n",
    "\n",
    "\n",
    "    # order of labels in `target_names` can be different from `categories`\n",
    "    target_names = data_train.target_names\n",
    "\n",
    "    # split target in a training set and a test set\n",
    "    y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "    # Extracting features from the training data using a sparse vectorizer\n",
    "    t0 = time()\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True, max_df=0.5, min_df=5, stop_words=\"english\"\n",
    "    )\n",
    "\n",
    "    X_train = vectorizer.fit_transform(data_train.data)\n",
    "    duration_train = time() - t0\n",
    "\n",
    "    # Extracting features from the test data using the same vectorizer\n",
    "    t0 = time()\n",
    "    X_test = vectorizer.transform(data_test.data)\n",
    "    duration_test = time() - t0\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    if verbose:\n",
    "        # compute size of loaded data\n",
    "        data_train_size_mb = size_mb(data_train.data)\n",
    "        data_test_size_mb = size_mb(data_test.data)\n",
    "\n",
    "        print(\n",
    "            f\"{len(data_train.data)} documents - \"\n",
    "            f\"{data_train_size_mb:.2f}MB (training set)\"\n",
    "        )\n",
    "        print(f\"{len(data_test.data)} documents - {data_test_size_mb:.2f}MB (test set)\")\n",
    "        print(f\"{len(target_names)} categories\")\n",
    "        print(\n",
    "            f\"vectorize training done in {duration_train:.3f}s \"\n",
    "            f\"at {data_train_size_mb / duration_train:.3f}MB/s\"\n",
    "        )\n",
    "        print(f\"n_samples: {X_train.shape[0]}, n_features: {X_train.shape[1]}\")\n",
    "        print(\n",
    "            f\"vectorize testing done in {duration_test:.3f}s \"\n",
    "            f\"at {data_test_size_mb / duration_test:.3f}MB/s\"\n",
    "        )\n",
    "        print(f\"n_samples: {X_test.shape[0]}, n_features: {X_test.shape[1]}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, feature_names, target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e73798a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi 713\n",
      "1073 documents - 1.33MB (training set)\n",
      "713 documents - 0.81MB (test set)\n",
      "2 categories\n",
      "vectorize training done in 0.142s at 9.337MB/s\n",
      "n_samples: 1073, n_features: 3420\n",
      "vectorize testing done in 0.089s at 9.148MB/s\n",
      "n_samples: 713, n_features: 3420\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, feature_names, target_names = load_dataset(\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e82a99",
   "metadata": {},
   "source": [
    "trying to replicate unbalanced testing (90/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1b3416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cats = [\n",
    "    \"alt.atheism\",\n",
    "    \"sci.space\",\n",
    "]\n",
    "chosen_seed = 200\n",
    "\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode(\"utf-8\")) for s in docs) / 1e6\n",
    "\n",
    "\n",
    "def load_dataset(verbose=False, remove=()):\n",
    "    \"\"\"Load and vectorize the 20 newsgroups dataset.\"\"\"\n",
    "\n",
    "    data_train = fetch_20newsgroups(\n",
    "        subset=\"train\",\n",
    "        categories=cats,\n",
    "        shuffle=True,\n",
    "        random_state=chosen_seed,\n",
    "        remove=('headers', 'footers', 'quotes'),\n",
    "    )\n",
    "\n",
    "    data_test = fetch_20newsgroups(\n",
    "        subset=\"test\",\n",
    "        categories=cats,\n",
    "        shuffle=True,\n",
    "        random_state=chosen_seed,\n",
    "        remove=('headers', 'footers', 'quotes'),\n",
    "    )\n",
    "    \n",
    "    # Separate ham and phishing\n",
    "    ham = [(text, label) for text, label in zip(data_test.data, data_test.target) if label == 0]\n",
    "    phishing = [(text, label) for text, label in zip(data_test.data, data_test.target) if label == 1]\n",
    "\n",
    "    # Choose size for imbalanced test set (e.g., total 2000 samples)\n",
    "    ham_n = int(len(data_test.data)*0.90)\n",
    "    phishing_n = len(data_test.data)-ham_n\n",
    "    print(\"hi\", ham_n+phishing_n)\n",
    "    np.random.seed(chosen_seed)\n",
    "\n",
    "    rng = np.random.default_rng(seed=chosen_seed) \n",
    "    ham_sample = rng.choice(ham, size=ham_n, replace=True)\n",
    "    phishing_sample = rng.choice(phishing, size=phishing_n, replace=True)\n",
    "\n",
    "    # Combine and shuffle\n",
    "    test_combined = list(ham_sample) + list(phishing_sample)\n",
    "    np.random.shuffle(test_combined)\n",
    "\n",
    "    # Unzip into X and y\n",
    "    X_test_raw, y_test = zip(*test_combined)\n",
    "    X_test_raw = list(X_test_raw)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # order of labels in `target_names` can be different from `categories`\n",
    "    target_names = data_train.target_names\n",
    "\n",
    "    # split target in a training set and a test set\n",
    "    y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "    # Extracting features from the training data using a sparse vectorizer\n",
    "    t0 = time()\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True, max_df=0.5, min_df=5, stop_words=\"english\"\n",
    "    )\n",
    "\n",
    "    X_train = vectorizer.fit_transform(data_train.data)\n",
    "    duration_train = time() - t0\n",
    "\n",
    "    # Extracting features from the test data using the same vectorizer\n",
    "    t0 = time()\n",
    "    X_test = vectorizer.transform(X_test_raw)\n",
    "    duration_test = time() - t0\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    if verbose:\n",
    "        # compute size of loaded data\n",
    "        data_train_size_mb = size_mb(data_train.data)\n",
    "        data_test_size_mb = size_mb(data_test.data)\n",
    "\n",
    "        print(\n",
    "            f\"{len(data_train.data)} documents - \"\n",
    "            f\"{data_train_size_mb:.2f}MB (training set)\"\n",
    "        )\n",
    "        print(f\"{len(data_test.data)} documents - {data_test_size_mb:.2f}MB (test set)\")\n",
    "        print(f\"{len(target_names)} categories\")\n",
    "        print(\n",
    "            f\"vectorize training done in {duration_train:.3f}s \"\n",
    "            f\"at {data_train_size_mb / duration_train:.3f}MB/s\"\n",
    "        )\n",
    "        print(f\"n_samples: {X_train.shape[0]}, n_features: {X_train.shape[1]}\")\n",
    "        print(\n",
    "            f\"vectorize testing done in {duration_test:.3f}s \"\n",
    "            f\"at {data_test_size_mb / duration_test:.3f}MB/s\"\n",
    "        )\n",
    "        print(f\"n_samples: {X_test.shape[0]}, n_features: {X_test.shape[1]}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, feature_names, target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f8f70ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.utils.extmath import density\n",
    "\n",
    "\n",
    "def benchmark(clf, custom_name=False):\n",
    "    print(\"_\" * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(f\"train time: {train_time:.3}s\")\n",
    "\n",
    "    t0 = time()\n",
    "    y_pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(f\"test time:  {test_time:.3}s\")\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "    precision = metrics.precision_score(y_true=y_test, y_pred=y_pred, pos_label=1)\n",
    "    recall = metrics.recall_score(y_true=y_test, y_pred=y_pred, pos_label=1)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(f\"Precision: {precision*100:.2f}%\")    # true positives/(true positives+false positives) --> out of all emails predicted as phishing, how many were phishing?\n",
    "    print(f\"Recall: {recall*100:.2f}%\")          # true positives/(true positives+false negatives) --> how many phishing were caught out of the total phishing amount?\n",
    "\n",
    "\n",
    "    if hasattr(clf, \"coef_\"):\n",
    "        print(f\"dimensionality: {clf.coef_.shape[1]}\")\n",
    "        print(f\"density: {density(clf.coef_)}\")\n",
    "        print()\n",
    "\n",
    "    print()\n",
    "    if custom_name:\n",
    "        clf_descr = str(custom_name)\n",
    "    else:\n",
    "        clf_descr = clf.__class__.__name__\n",
    "    return clf_descr, accuracy, train_time, test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cab43931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Logistic Regression\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LogisticRegression(C=5, max_iter=1000)\n",
      "train time: 0.0119s\n",
      "test time:  0.000283s\n",
      "Accuracy: 87.66%\n",
      "Precision: 85.25%\n",
      "Recall: 93.91%\n",
      "dimensionality: 3420\n",
      "density: 1.0\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Complement naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "ComplementNB(alpha=0.1)\n",
      "train time: 0.00192s\n",
      "test time:  0.000338s\n",
      "Accuracy: 88.64%\n",
      "Precision: 91.73%\n",
      "Recall: 87.31%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "results = []\n",
    "for clf, name in (\n",
    "    (LogisticRegression(C=5, max_iter=1000), \"Logistic Regression\"),\n",
    "    (ComplementNB(alpha=0.1), \"Complement naive Bayes\"),    # Sparse naive Bayes classifier\n",
    "    # naive Bayes classifier outperforms logistic regression while having a shorter training time\n",
    "    \n",
    "):\n",
    "    print(\"=\" * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf, name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
